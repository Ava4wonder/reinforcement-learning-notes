A constantly evolving list of notes and summaries of the Reinforcement Learning papers.

# Deep Reinforcement Learning
## Year 2017
**Neural Episodic Control**
  - Google DeepMind
  - Alexander Pritzel, Benigno Uria, Sriram Srinivasan, Adria Puigdom, Oriol Vinyals, Demis Hassabis, Daan Wierstra, Charles Blundell
  - [[arXiv](https://arxiv.org/abs/1703.01988v1)], [[pdf](https://arxiv.org/pdf/1703.01988v1.pdf)]
  - :pencil: [**Notes**](./notes/nec-agent.md)
  - **Summary.** Neural Episidic Control (NEC) agent much more data efficient (in small data regime) compared to other state-of-the-art RL agents. DQN with Prioritized Replay can reach NEC performance at 5 millions of frames, only after 40 millions of frames. However, final perfomance in almost all environments is still worse than other state-of-the-art agents can obtain.

## Year 2016
**UNREAL agent: Reinforcement Learning with unsupervised auxiliary tasks**
  - Google DeepMind
  - Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul, Joel Z Leibo, David Silver & Koray Kavukcuoglu
  - [[arXiv](https://arxiv.org/abs/1611.05397)], [[pdf](https://arxiv.org/pdf/1611.05397.pdf)] 
  - :pencil: [**Notes**](./notes/unreal-agent.md)
  
  
:rocket: **A3C agent: Asynchronous Methods for Deep Reinforcement Learning**
  - Google DeepMind
  - Volodymyr Mnih, Adrià Puigdomènech Badia, Mehdi Mirza, Alex Graves, Tim Harley, Timothy P. Lillicrap, David Silver, Koray Kavukcuoglu
  - [[arXiv](https://arxiv.org/abs/1602.01783v2)], [[pdf](https://arxiv.org/pdf/1602.01783v2.pdf)]
  - :pencil: [**Notes**](./notes/a3c-agent.md)

## Year 2015
**Prioritized Experience Replay**
  - Google DeepMind
  - Tom Schaul, John Quan, Ioannis Antonoglou, David Silver
  - [[arXiv](https://arxiv.org/abs/1511.05952v4)], [[pdf](https://arxiv.org/pdf/1511.05952v4.pdf)]
  - :pencil: [**Notes**](./notes/prioritized-exp-replay.md)
